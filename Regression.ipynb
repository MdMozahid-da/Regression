{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Regression"
      ],
      "metadata": {
        "id": "y2PqMxXYGWXN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        " - Simple linear regression is a statistical method used to model the relationship between a dependent variable and one independent variable. It assumes a linear relationship between the two variables and fits a straight line to represent this association. The equation used is\n",
        "𝑌\n",
        "=\n",
        "𝑚\n",
        "𝑋\n",
        "+\n",
        "𝑐\n",
        ", where\n",
        "𝑚\n",
        " is the slope and\n",
        "𝑐\n",
        " is the intercept. This method is widely used in predictive analysis, trend forecasting, and assessing correlations between variables. For example, predicting sales based on advertising budget can be done using simple linear regression."
      ],
      "metadata": {
        "id": "EjbwdUWIGlzv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the key assumptions of Simple Linear Regression?\n",
        " - For simple linear regression to be valid, certain assumptions must be met:\n",
        "\n",
        "        - The relationship between the independent and dependent variables must be linear.\n",
        "\n",
        "        - Residuals (errors) should be normally distributed.\n",
        "\n",
        "        - There should be no correlation between residuals (independence).\n",
        "\n",
        "        - The variance of residuals must remain constant (homoscedasticity).\n",
        "\n",
        "        - There should be no significant outliers influencing the results. Violating these assumptions can lead to inaccurate predictions and misleading interpretations of data."
      ],
      "metadata": {
        "id": "2CjSIaPyHPNF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What does the coefficient\n",
        "𝑚\n",
        " represent in the equation\n",
        "𝑌\n",
        "=\n",
        "𝑚\n",
        "𝑋\n",
        "+\n",
        "𝑐\n",
        "?\n",
        " -   The coefficient\n",
        "𝑚\n",
        " in the equation represents the slope of the regression line, indicating how much the dependent variable changes for each unit increase in the independent variable. If\n",
        "𝑚\n",
        " is positive, an increase in\n",
        "𝑋\n",
        " leads to an increase in\n",
        "𝑌\n",
        ". If it's negative,\n",
        "𝑌\n",
        " decreases as\n",
        "𝑋\n",
        " increases. The magnitude of\n",
        "𝑚\n",
        " tells us the strength of the relationship. For example, if a store finds that increasing advertising spending by $100 leads to a $500 sales increase, then the slope\n",
        "𝑚\n",
        " is 5."
      ],
      "metadata": {
        "id": "CRn87E1pHkTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What does the intercept\n",
        "𝑐\n",
        " represent in the equation\n",
        "𝑌\n",
        "=\n",
        "𝑚\n",
        "𝑋\n",
        "+\n",
        "𝑐\n",
        "?\n",
        "\n",
        "  - The intercept\n",
        "𝑐\n",
        " represents the value of the dependent variable (Y) when the independent variable (X) is zero. It determines where the regression line crosses the Y-axis. In practical applications, the intercept gives the baseline value of\n",
        "𝑌\n",
        " when there is no influence from\n",
        "𝑋\n",
        ". For example, if a company's revenue is modeled using Simple Linear Regression,\n",
        "𝑐\n",
        " would represent the revenue when zero products are sold, providing a reference point for predictions."
      ],
      "metadata": {
        "id": "iPSPm87ipGJg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do we calculate the slope\n",
        "𝑚\n",
        " in Simple Linear Regression?\n",
        "\n",
        "\n",
        " - The slope\n",
        "  𝑚\n",
        "    in Simple Linear Regression is calculated using the formula:\n",
        "\n",
        "𝑚\n",
        "=\n",
        "𝑛\n",
        "(\n",
        "∑\n",
        "𝑋\n",
        "𝑌\n",
        ")\n",
        "−\n",
        "(\n",
        "∑\n",
        "𝑋\n",
        ")\n",
        "(\n",
        "∑\n",
        "𝑌\n",
        ")\n",
        "𝑛\n",
        "(\n",
        "∑\n",
        "𝑋\n",
        "2\n",
        ")\n",
        "−\n",
        "(\n",
        "∑\n",
        "𝑋\n",
        ")\n",
        "2\n",
        "\n",
        " where:\n",
        "\n",
        " 𝑛\n",
        " is the number of observations,\n",
        "\n",
        "∑\n",
        "𝑋\n",
        "𝑌\n",
        " is the sum of the product of\n",
        "𝑋\n",
        " and\n",
        "𝑌\n",
        ",\n",
        "\n",
        "∑\n",
        "𝑋\n",
        " is the sum of all\n",
        "𝑋\n",
        " values,\n",
        "\n",
        "∑\n",
        "𝑌\n",
        " is the sum of all\n",
        "𝑌\n",
        " values,\n",
        "\n",
        "∑\n",
        "𝑋\n",
        "2\n",
        " is the sum of the squared values of\n",
        "𝑋\n",
        ". This formula helps compute the best-fitting line by minimizing errors."
      ],
      "metadata": {
        "id": "SLD_tLE3pRSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        " - The least squares method is used to find the best-fitting regression line by minimizing the sum of the squared errors (differences between actual values and predicted values). Squaring the errors prevents them from canceling out and ensures a more precise fit. The least squares method provides the most optimal estimates for the regression coefficients, making it widely used in predictive modeling and forecasting."
      ],
      "metadata": {
        "id": "ux_jNFuKpdHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        " -\n",
        "The coefficient of determination (R²) measures how well the regression line explains the variability of the dependent variable. It ranges from 0 to 1:\n",
        "\n",
        "     - 𝑅\n",
        "2\n",
        "=\n",
        "1\n",
        " → Perfect fit; all variance in\n",
        "𝑌\n",
        " is explained by\n",
        "𝑋\n",
        ".\n",
        "\n",
        "    - 𝑅\n",
        "2\n",
        "=\n",
        "0\n",
        " → No explanatory power;\n",
        "𝑋\n",
        " has no impact on\n",
        "𝑌\n",
        ". Higher\n",
        "𝑅\n",
        "2\n",
        " values suggest a strong relationship, whereas lower values indicate that factors other than\n",
        "𝑋\n",
        " affect\n",
        "𝑌\n",
        "."
      ],
      "metadata": {
        "id": "iyJ3QShPpnRp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is Multiple Linear Regression?\n",
        " -\n",
        "Multiple Linear Regression extends Simple Linear Regression by using two or more independent variables to predict the dependent variable. The equation is:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝑏\n",
        "0\n",
        "+\n",
        "𝑏\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "𝑏\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝑏\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "where\n",
        "𝑏\n",
        "0\n",
        " is the intercept, and\n",
        "𝑏\n",
        "1\n",
        ",\n",
        "𝑏\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑏\n",
        "𝑛\n",
        "      are the coefficients of the independent variables. Multiple Linear Regression allows for more complex predictions by considering multiple influencing factors simultaneously."
      ],
      "metadata": {
        "id": "6wvnl5T2p_Vz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        " -\n",
        "The key difference between Simple and Multiple Linear Regression is the number of independent variables:\n",
        "\n",
        "    - Simple Linear Regression → Uses only one independent variable to predict the dependent variable.\n",
        "\n",
        "    - Multiple Linear Regression → Uses two or more independent variables, providing a more detailed model. Multiple Linear Regression is especially useful when a single variable cannot fully explain the dependent variable."
      ],
      "metadata": {
        "id": "DQEdN-2wqMFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is Heteroscedasticity, and how does it affect regression models?\n",
        " - Heteroscedasticity occurs when the variance of residuals is not constant across all levels of the independent variable. In regression models, this can lead to inefficient parameter estimates and unreliable hypothesis tests. Graphically, it often appears as a \"funnel shape\" in residual plots, where residuals increase as the independent variable values increase. To address this issue, transformations like log transformations or weighted regression techniques can be applied to stabilize variance and improve model reliability."
      ],
      "metadata": {
        "id": "iRME-UgfqhQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        " - Heteroscedasticity occurs when the variance of residuals is not constant across values of the independent variable.\n",
        "\n",
        "     - This violates the assumption of homoscedasticity and can lead to biased standard errors, making statistical tests unreliable.\n",
        "\n",
        "     - To detect heteroscedasticity, residual plots are used, and solutions like log transformations or weighted least squares regression can be applied."
      ],
      "metadata": {
        "id": "8CNW8FK6qoAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        " - Techniques to reduce multicollinearity:\n",
        "\n",
        "     - Feature Selection → Remove correlated independent variables.\n",
        "\n",
        "     - Principal Component Analysis (PCA) → Convert correlated features into uncorrelated components.\n",
        "\n",
        "     - Regularization (Ridge/Lasso Regression) → Apply penalties to large coefficients.\n",
        "\n",
        "     - Increasing Sample Size → Reduces coefficient variability."
      ],
      "metadata": {
        "id": "pjA01fUirGqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        " - To use categorical variables in regression:\n",
        "\n",
        "     - Use One-Hot Encoding (dummy variables) for nominal categories.\n",
        "\n",
        "     - Use Label Encoding for ordinal variables.\n",
        "\n",
        "     - Consider Target Encoding for high-cardinality categories.\n",
        "\n",
        "     - Drop one dummy variable per categorical variable to avoid the dummy variable trap.\n",
        "Transforming categorical variables allows regression models to include qualitative data."
      ],
      "metadata": {
        "id": "IGZ0sFn8rUBJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        " - Interaction terms allow the effect of one independent variable to depend on the level of another. For example, if both X1 and X2 affect Y, an interaction term (X1 × X2) captures their combined effect. Including such terms makes the model more flexible and capable of modeling complex relationships, though it also increases complexity and potential multicollinearity."
      ],
      "metadata": {
        "id": "di56GSr2rcII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        " - In Simple Linear Regression, the intercept is the value of Y when X = 0.\n",
        "In Multiple Linear Regression, the intercept represents the value of Y when all independent variables are zero, which may not always be realistic or interpretable. The meaning of the intercept depends heavily on whether zero is within the data range for all predictors."
      ],
      "metadata": {
        "id": "FifObhslriiI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        " - The slope represents the change in the dependent variable Y for a one-unit change in the independent variable X, assuming all other variables are constant. In MLR, each slope tells the unique contribution of its respective predictor. Larger absolute values of slopes imply stronger relationships, and incorrect slope estimates lead to poor predictions."
      ],
      "metadata": {
        "id": "O-GSbYCirp14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        " - The intercept gives a baseline prediction for Y when all predictors are zero. It helps in anchoring the regression line or plane. Though sometimes not meaningful in real-world terms (e.g., zero income or age), it's necessary mathematically. In model interpretation, it's often ignored unless it has practical significance in the problem domain.\n",
        "\n"
      ],
      "metadata": {
        "id": "0pVvh4zWsGhk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What are the limitations of using R² as a sole measure of model performance?\n",
        " - While R² shows the proportion of variance explained, it has limitations:\n",
        "\n",
        "     - It always increases with more variables, even if they’re not useful.\n",
        "\n",
        "     - It doesn't indicate whether a model is statistically significant.\n",
        "\n",
        "     - It doesn’t reveal overfitting.\n",
        "\n",
        "      - t fails to assess predictive power on unseen data.\n",
        "Hence, metrics like Adjusted R², RMSE, or cross-validation scores are also important."
      ],
      "metadata": {
        "id": "85ELj75zCWPs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        " - A large standard error indicates that the coefficient estimate is unstable or imprecise. This could mean the variable isn’t significant, or there may be multicollinearity or high variance in the data. If the standard error is large relative to the coefficient itself, the corresponding p-value may be high, meaning the variable isn’t statistically significant."
      ],
      "metadata": {
        "id": "vMR0rDSDCa1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        " - Residual plots (residuals vs. fitted values) help detect heteroscedasticity. If residuals fan out or form patterns rather than a random spread, heteroscedasticity is likely present. It's important to address because it affects the reliability of standard errors and confidence intervals, potentially leading to incorrect inferences. Fixing it may involve using transformations or robust standard errors."
      ],
      "metadata": {
        "id": "218OsxQsC3H9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        " - A high R² with a low Adjusted R² suggests that some independent variables don’t significantly contribute to explaining the variation in the dependent variable. While R² always increases with added predictors, Adjusted R² penalizes irrelevant ones. A drop in Adjusted R² indicates overfitting—adding too many variables that don’t improve the model's predictive power."
      ],
      "metadata": {
        "id": "j86L_5QCC4jh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        " - Scaling ensures that all variables contribute equally to the model. In models with regularization (e.g., Ridge, Lasso), or when variables have very different units (e.g., age vs. income), lack of scaling can cause the algorithm to favor one variable over others. It also helps improve convergence speed and model stability during training. Standardization (zero mean, unit variance) is commonly used."
      ],
      "metadata": {
        "id": "m013qR29C-bb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is polynomial regression?\n",
        " - Polynomial regression is a type of regression that models the relationship between the independent variable and the dependent variable as an nth-degree polynomial. The equation is:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝑏\n",
        "0\n",
        "+\n",
        "𝑏\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝑏\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝑏\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "Y=b\n",
        "0\n",
        "​\n",
        " +b\n",
        "1\n",
        "​\n",
        " X+b\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        " +⋯+b\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "\n",
        "  - It allows fitting curves rather than straight lines, making it useful when the data shows nonlinear trends. It is still a linear model in terms of the coefficients."
      ],
      "metadata": {
        "id": "gE8O9FYqDCsG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How does polynomial regression differ from linear regression?\n",
        " - While linear regression models a straight-line relationship, polynomial regression models a curved one by including powers of the predictor variable(s). Polynomial regression can fit more complex patterns in the data but is more prone to overfitting if the degree of the polynomial is too high. Linear regression is a special case (degree = 1)."
      ],
      "metadata": {
        "id": "IGqrBSAuDGd6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. When is polynomial regression used?\n",
        " - It is used when the relationship between the independent and dependent variables is nonlinear, and a straight line cannot accurately capture the trend. Common use cases include:\n",
        "\n",
        "     - Growth curves (biology, economics),\n",
        "\n",
        "      - Sensor data (where trends are curved),\n",
        "\n",
        "     - Engineering measurements,\n",
        "\n",
        "     - :Modeling seasonal effects.\n",
        "Polynomial regression captures more complexity while remaining relatively simple to interpret."
      ],
      "metadata": {
        "id": "viH4FUnfDKen"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. What is the general equation for polynomial regression?\n",
        " - The general form is:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝑏\n",
        "0\n",
        "+\n",
        "𝑏\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝑏\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "𝑏\n",
        "3\n",
        "𝑋\n",
        "3\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝑏\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "+\n",
        "𝜀\n",
        "Y=b\n",
        "0\n",
        "​\n",
        " +b\n",
        "1\n",
        "​\n",
        " X+b\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        " +b\n",
        "3\n",
        "​\n",
        " X\n",
        "3\n",
        " +⋯+b\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        " +ε\n",
        "Here:\n",
        "\n",
        "𝑌\n",
        "Y is the dependent variable,\n",
        "\n",
        "𝑋\n",
        "X is the independent variable,\n",
        "\n",
        "𝑏\n",
        "0\n",
        ",\n",
        "𝑏\n",
        "1\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑏\n",
        "𝑛\n",
        "b\n",
        "0\n",
        "​\n",
        " ,b\n",
        "1\n",
        "​\n",
        " ,…,b\n",
        "n\n",
        "​\n",
        "  are coefficients,\n",
        "\n",
        "𝑛\n",
        "n is the degree of the polynomial,\n",
        "\n",
        "𝜀\n",
        "ε is the error term."
      ],
      "metadata": {
        "id": "NYdqcMAHDUYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Can polynomial regression be applied to multiple variables?\n",
        " - Yes, it can be extended to multiple variables, where the model includes interaction terms and polynomial terms for each variable. For example:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝑏\n",
        "0\n",
        "+\n",
        "𝑏\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "𝑏\n",
        "2\n",
        "𝑋\n",
        "1\n",
        "2\n",
        "+\n",
        "𝑏\n",
        "3\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "𝑏\n",
        "4\n",
        "𝑋\n",
        "2\n",
        "2\n",
        "+\n",
        "𝑏\n",
        "5\n",
        "𝑋\n",
        "1\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "…\n",
        "Y=b\n",
        "0\n",
        "​\n",
        " +b\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +b\n",
        "2\n",
        "​\n",
        " X\n",
        "1\n",
        "2\n",
        "​\n",
        " +b\n",
        "3\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +b\n",
        "4\n",
        "​\n",
        " X\n",
        "2\n",
        "2\n",
        "​\n",
        " +b\n",
        "5\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +…\n",
        "This is called multivariate polynomial regression and allows for complex surface fitting, but it increases model complexity rapidly.\n",
        "\n"
      ],
      "metadata": {
        "id": "Q9ikWigYDX6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. What are the limitations of polynomial regression?\n",
        " -\n",
        "     - Overfitting: Higher-degree polynomials can fit training data too closely.\n",
        "\n",
        "     - Instability: Small changes in data can lead to large coefficient changes.\n",
        "\n",
        "     - Extrapolation issues: Predictions outside the data range can become extreme or meaningless.\n",
        "\n",
        "     - Interpretability: Higher-degree models are harder to explain.\n",
        "\n",
        "     - Computational cost increases with degree and number of variables."
      ],
      "metadata": {
        "id": "sgMaPk3RDbHV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        " - Use:\n",
        "\n",
        "     - Cross-validation (e.g., k-fold) to test generalizability.\n",
        "\n",
        "     - Adjusted R² to penalize unnecessary terms.\n",
        "\n",
        "     - AIC/BIC (Akaike/Bayesian Information Criteria) to balance fit and complexity.\n",
        "\n",
        "     - Residual plots to check for patterns.\n",
        "\n",
        "     - RMSE/MAE on validation data.\n",
        "Choosing the right degree helps avoid underfitting and overfitting."
      ],
      "metadata": {
        "id": "H_nGD9PdDfa2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Why is visualization important in polynomial regression?\n",
        " - Visualization:\n",
        "\n",
        "     - Helps understand the shape of the curve fitted by the model.\n",
        "\n",
        "     - Reveals overfitting (e.g., wiggly curves).\n",
        "\n",
        "     - Makes it easier to explain results to non-technical stakeholders.\n",
        "\n",
        "     - Helps assess the bias-variance tradeoff.\n",
        "Plotting actual vs predicted values or fitted curves makes complex models more interpretable and errors easier to spot."
      ],
      "metadata": {
        "id": "pDq6choGDjyb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. How is polynomial regression implemented in Python?"
      ],
      "metadata": {
        "id": "1mxQ6oKKDn1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Degree 3 polynomial regression\n",
        "model = make_pipeline(PolynomialFeatures(3), LinearRegression())\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "'''"
      ],
      "metadata": {
        "id": "PDPpTqeFIvcN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}